{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited version of AOC to make it more organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, preprocessing (using same code as for kickstarter as baseline, come back here to tweak later)\n",
    "\n",
    "Look into https://pypi.org/project/tweet-preprocessor/ for tweet processing later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #lemmatize with Spacy\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stopwords = stopwords.words('english') + \\\n",
    "    ['rt', 'https', 'http', 'amp', 'via', 'one', 'around', 'would', 'let', 'could', 'going', 'like', \n",
    "     'get', 'may', 'says', 'say', 'make', 'based', 'even', 'another', 'completely', 'thanks', 'way', \n",
    "     'find', 'used', 'thing', '2019', 'see', 'need', 'know', 'knows', 'think', 'thinks', 'take', 'new', \n",
    "     'day', 'days', 'john', 'mccain', 'johnmccain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_0.pickle', 'rb') as f:\n",
    "    df_0 = pickle.load(f)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_1.pickle', 'rb') as f:\n",
    "    df_1 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_3.pickle', 'rb') as f:\n",
    "    df_3 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_4.pickle', 'rb') as f:\n",
    "    df_4 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_5.pickle', 'rb') as f:\n",
    "    df_5 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_6.pickle', 'rb') as f:\n",
    "    df_6 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_7.pickle', 'rb') as f:\n",
    "    df_7 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_8.pickle', 'rb') as f:\n",
    "    df_8 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_9.pickle', 'rb') as f:\n",
    "    df_9 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_10.pickle', 'rb') as f:\n",
    "    df_10 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_11.pickle', 'rb') as f:\n",
    "    df_11 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_12.pickle', 'rb') as f:\n",
    "    df_12 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_13.pickle', 'rb') as f:\n",
    "    df_13 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_14.pickle', 'rb') as f:\n",
    "    df_14 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/df_15.pickle', 'rb') as f:\n",
    "    df_15 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['final_text'] = np.where((df_0['retweet_text'].isnull() == False), df_0['retweet_text'], df_0['main_text'])\n",
    "df_1['final_text'] = np.where((df_1['retweet_text'].isnull() == False), df_1['retweet_text'], df_1['main_text'])\n",
    "df_3['final_text'] = np.where((df_3['retweet_text'].isnull() == False), df_3['retweet_text'], df_3['main_text'])\n",
    "df_4['final_text'] = np.where((df_4['retweet_text'].isnull() == False), df_4['retweet_text'], df_4['main_text'])\n",
    "df_5['final_text'] = np.where((df_5['retweet_text'].isnull() == False), df_5['retweet_text'], df_5['main_text'])\n",
    "df_6['final_text'] = np.where((df_6['retweet_text'].isnull() == False), df_6['retweet_text'], df_6['main_text'])\n",
    "df_7['final_text'] = np.where((df_7['retweet_text'].isnull() == False), df_7['retweet_text'], df_7['main_text'])\n",
    "df_8['final_text'] = np.where((df_8['retweet_text'].isnull() == False), df_8['retweet_text'], df_8['main_text'])\n",
    "df_9['final_text'] = np.where((df_9['retweet_text'].isnull() == False), df_9['retweet_text'], df_9['main_text'])\n",
    "df_10['final_text'] = np.where((df_10['retweet_text'].isnull() == False), df_10['retweet_text'], df_10['main_text'])\n",
    "df_11['final_text'] = np.where((df_11['retweet_text'].isnull() == False), df_11['retweet_text'], df_11['main_text'])\n",
    "df_12['final_text'] = np.where((df_12['retweet_text'].isnull() == False), df_12['retweet_text'], df_12['main_text'])\n",
    "df_13['final_text'] = np.where((df_13['retweet_text'].isnull() == False), df_13['retweet_text'], df_13['main_text'])\n",
    "df_14['final_text'] = np.where((df_14['retweet_text'].isnull() == False), df_14['retweet_text'], df_14['main_text'])\n",
    "df_15['final_text'] = np.where((df_15['retweet_text'].isnull() == False), df_15['retweet_text'], df_15['main_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN ALL BELOW ONCE MARVEL DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['tweet_processed'] = df_0['final_text'].apply(lambda x:pre_process(x))\n",
    "df_1['tweet_processed'] = df_1['final_text'].apply(lambda x:pre_process(x))\n",
    "df_3['tweet_processed'] = df_3['final_text'].apply(lambda x:pre_process(x))\n",
    "df_4['tweet_processed'] = df_4['final_text'].apply(lambda x:pre_process(x))\n",
    "df_5['tweet_processed'] = df_5['final_text'].apply(lambda x:pre_process(x))\n",
    "df_6['tweet_processed'] = df_6['final_text'].apply(lambda x:pre_process(x))\n",
    "df_7['tweet_processed'] = df_7['final_text'].apply(lambda x:pre_process(x))\n",
    "df_8['tweet_processed'] = df_8['final_text'].apply(lambda x:pre_process(x))\n",
    "df_9['tweet_processed'] = df_9['final_text'].apply(lambda x:pre_process(x))\n",
    "df_10['tweet_processed'] = df_10['final_text'].apply(lambda x:pre_process(x))\n",
    "df_11['tweet_processed'] = df_11['final_text'].apply(lambda x:pre_process(x))\n",
    "df_12['tweet_processed'] = df_12['final_text'].apply(lambda x:pre_process(x))\n",
    "df_13['tweet_processed'] = df_13['final_text'].apply(lambda x:pre_process(x))\n",
    "df_14['tweet_processed'] = df_14['final_text'].apply(lambda x:pre_process(x))\n",
    "df_15['tweet_processed'] = df_15['final_text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_0_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_0, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_1_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_1, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_3_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_3, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_4_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_4, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_5_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_5, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_6_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_6, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_7_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_7, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_8_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_8, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_9_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_9, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_10_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_10, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_11_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_11, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_12_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_12, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_13_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_13, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_14_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_14, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/mccain/community_pickles_pre_nlp/preproccessed_text/df_15_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_15, to_write)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = df_1['tweet_processed']\n",
    "X_1 = df_1['tweet_processed']\n",
    "X_3 = df_3['tweet_processed']\n",
    "X_4 = df_4['tweet_processed']\n",
    "X_5 = df_5['tweet_processed']\n",
    "X_6 = df_6['tweet_processed']\n",
    "X_7 = df_7['tweet_processed']\n",
    "X_8 = df_8['tweet_processed']\n",
    "X_9 = df_9['tweet_processed']\n",
    "X_10 = df_10['tweet_processed']\n",
    "X_11 = df_11['tweet_processed']\n",
    "X_12 = df_12['tweet_processed']\n",
    "X_13 = df_13['tweet_processed']\n",
    "X_14 = df_14['tweet_processed']\n",
    "X_15 = df_15['tweet_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_0 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_1 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_3 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_4 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_5 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_6 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_7 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_8 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_9 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_10 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_11 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_12 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_13 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_14 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_15 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_0 = tfidf_0.fit_transform(X_0)\n",
    "lsa_0 = TruncatedSVD(20)\n",
    "doc_topic_0 = lsa_0.fit_transform(bag_of_words_0)\n",
    "lsa_0.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_0, tfidf_0.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_1 = tfidf_1.fit_transform(X_1)\n",
    "lsa_1 = TruncatedSVD(20)\n",
    "doc_topic_1 = lsa_1.fit_transform(bag_of_words_1)\n",
    "lsa_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_1, tfidf_1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_3 = tfidf_3.fit_transform(X_3)\n",
    "lsa_3 = TruncatedSVD(20)\n",
    "doc_topic_3 = lsa_3.fit_transform(bag_of_words_3)\n",
    "lsa_3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_3, tfidf_3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_4 = tfidf_4.fit_transform(X_4)\n",
    "lsa_4 = TruncatedSVD(20)\n",
    "doc_topic_4 = lsa_4.fit_transform(bag_of_words_4)\n",
    "lsa_4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_4, tfidf_4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_5 = tfidf_5.fit_transform(X_5)\n",
    "lsa_5 = TruncatedSVD(20)\n",
    "doc_topic_5 = lsa_5.fit_transform(bag_of_words_5)\n",
    "lsa_5.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_5, tfidf_5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_6 = tfidf_6.fit_transform(X_6)\n",
    "lsa_6 = TruncatedSVD(20)\n",
    "doc_topic_6 = lsa_6.fit_transform(bag_of_words_6)\n",
    "lsa_6.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_6, tfidf_6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_7 = tfidf_7.fit_transform(X_7)\n",
    "lsa_7 = TruncatedSVD(20)\n",
    "doc_topic_7 = lsa_7.fit_transform(bag_of_words_7)\n",
    "lsa_7.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_7, tfidf_7.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_8 = tfidf_8.fit_transform(X_8)\n",
    "lsa_8 = TruncatedSVD(20)\n",
    "doc_topic_8 = lsa_8.fit_transform(bag_of_words_8)\n",
    "lsa_8.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_8, tfidf_8.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_9 = tfidf_9.fit_transform(X_9)\n",
    "lsa_9 = TruncatedSVD(20)\n",
    "doc_topic_9 = lsa_9.fit_transform(bag_of_words_9)\n",
    "lsa_9.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_9, tfidf_9.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_10 = tfidf_10.fit_transform(X_10)\n",
    "lsa_10 = TruncatedSVD(20)\n",
    "doc_topic_10 = lsa_10.fit_transform(bag_of_words_10)\n",
    "lsa_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_10, tfidf_10.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_11 = tfidf_11.fit_transform(X_11)\n",
    "lsa_11 = TruncatedSVD(20)\n",
    "doc_topic_11 = lsa_11.fit_transform(bag_of_words_11)\n",
    "lsa_11.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_11, tfidf_11.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_12 = tfidf_12.fit_transform(X_12)\n",
    "lsa_12 = TruncatedSVD(20)\n",
    "doc_topic_12 = lsa_12.fit_transform(bag_of_words_12)\n",
    "lsa_12.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_12, tfidf_12.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_13 = tfidf_13.fit_transform(X_13)\n",
    "lsa_13 = TruncatedSVD(20)\n",
    "doc_topic_13 = lsa_13.fit_transform(bag_of_words_13)\n",
    "lsa_13.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_13, tfidf_13.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_14 = tfidf_14.fit_transform(X_14)\n",
    "lsa_14 = TruncatedSVD(20)\n",
    "doc_topic_14 = lsa_14.fit_transform(bag_of_words_14)\n",
    "lsa_14.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_14, tfidf_14.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_15 = tfidf_15.fit_transform(X_15)\n",
    "lsa_15 = TruncatedSVD(20)\n",
    "doc_topic_15 = lsa_15.fit_transform(bag_of_words_15)\n",
    "lsa_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_15, tfidf_15.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have my top words in each LSA object w/ 20 topics each. I'm going to pickle each model and then use a new file to:\n",
    " - Find which tweets score the highest in each topic to get examples\n",
    " - Name communities\n",
    " - Sentiment analysis (maybe weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_0.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_0, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_1.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_1, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_3.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_3, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_4.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_4, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_5.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_5, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_6.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_6, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_7.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_7, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_8.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_8, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_9.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_9, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_10.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_10, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_11.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_11, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_12.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_12, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_13.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_13, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_14.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_14, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_mccain/lsa_15.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_15, to_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF Models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nmf_0 = NMF(20)\n",
    "doc_topic_NMF_0 = nmf_0.fit_transform(bag_of_words_0)\n",
    "print(nmf_0.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_0, tfidf_0.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_1 = NMF(20)\n",
    "doc_topic_NMF_1 = nmf_1.fit_transform(bag_of_words_1)\n",
    "print(nmf_1.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_1, tfidf_1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_3 = NMF(20)\n",
    "doc_topic_NMF_3 = nmf_3.fit_transform(bag_of_words_3)\n",
    "print(nmf_3.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_3, tfidf_3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_4 = NMF(20)\n",
    "doc_topic_NMF_4 = nmf_4.fit_transform(bag_of_words_4)\n",
    "print(nmf_4.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_4, tfidf_4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_5 = NMF(20)\n",
    "doc_topic_NMF_5 = nmf_5.fit_transform(bag_of_words_5)\n",
    "print(nmf_5.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_5, tfidf_5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_6 = NMF(20)\n",
    "doc_topic_NMF_6 = nmf_6.fit_transform(bag_of_words_6)\n",
    "print(nmf_6.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_6, tfidf_6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_7 = NMF(20)\n",
    "doc_topic_NMF_7 = nmf_7.fit_transform(bag_of_words_7)\n",
    "print(nmf_7.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_7, tfidf_7.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_8 = NMF(20)\n",
    "doc_topic_NMF_8 = nmf_8.fit_transform(bag_of_words_8)\n",
    "print(nmf_8.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_8, tfidf_8.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_9 = NMF(20)\n",
    "doc_topic_NMF_9 = nmf_9.fit_transform(bag_of_words_9)\n",
    "print(nmf_9.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_9, tfidf_9.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_10 = NMF(20)\n",
    "doc_topic_NMF_10 = nmf_10.fit_transform(bag_of_words_10)\n",
    "print(nmf_10.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_10, tfidf_10.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_11 = NMF(20)\n",
    "doc_topic_NMF_11 = nmf_11.fit_transform(bag_of_words_11)\n",
    "print(nmf_11.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_11, tfidf_11.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_12 = NMF(20)\n",
    "doc_topic_NMF_12 = nmf_12.fit_transform(bag_of_words_12)\n",
    "print(nmf_12.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_12, tfidf_12.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_13 = NMF(20)\n",
    "doc_topic_NMF_13 = nmf_13.fit_transform(bag_of_words_13)\n",
    "print(nmf_13.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_13, tfidf_13.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_14 = NMF(20)\n",
    "doc_topic_NMF_14 = nmf_14.fit_transform(bag_of_words_14)\n",
    "print(nmf_14.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_14, tfidf_14.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_15 = NMF(20)\n",
    "doc_topic_NMF_15 = nmf_15.fit_transform(bag_of_words_15)\n",
    "print(nmf_15.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_15, tfidf_15.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_0.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_0, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_1.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_1, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_3.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_3, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_4.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_4, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_5.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_5, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_6.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_6, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_7.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_7, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_8.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_8, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_9.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_9, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_10.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_10, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_11.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_11, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_12.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_12, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_13.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_13, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_14.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_14, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_mccain/nmf_15.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_15, to_write)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Topic Modeling on ALL text data, both NMF and LSA, on various topic counts, to feed into DBScan / KMeans for cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Creating list of the processed text files to merge, so it's all in the same format and can re-use code\n",
    "# dfs_to_merge = [df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_26, df_40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34997, 10)\n",
      "(1446, 10)\n",
      "(8345, 10)\n",
      "(9346, 10)\n",
      "(224793, 10)\n",
      "(237746, 10)\n",
      "(190998, 10)\n",
      "(135612, 10)\n",
      "(8192, 10)\n",
      "(425343, 10)\n",
      "(13157, 10)\n",
      "(2270, 10)\n",
      "(1141, 10)\n",
      "(2628, 10)\n",
      "1296014\n"
     ]
    }
   ],
   "source": [
    "# total = 0\n",
    "\n",
    "# for df in dfs_to_merge:\n",
    "#     print(df.shape)\n",
    "#     total += df.shape[0]\n",
    "    \n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_preprocessed = pd.concat(dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 10)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = df_all_preprocessed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>modularity_class</th>\n",
       "      <th>main_text</th>\n",
       "      <th>retweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>PageRank</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>final_text</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405812</th>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So @aoc answered a casting call... and now she’s in Congress! @RealJamesWoods and @realDonaldTrump have to see this!!! https://t.co/HBUW78XqAo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Mar 11 01:16:04 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>6936</td>\n",
       "      <td>So @aoc answered a casting call... and now she’s in Congress! @RealJamesWoods and @realDonaldTrump have to see this!!! https://t.co/HBUW78XqAo</td>\n",
       "      <td>so aoc answer a cast call and now -PRON- s in congress realjameswood and realdonaldtrump have to see this https t co hbuw xqao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712255</th>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So @aoc wants to raise our taxes and she hasn’t even paid her own! https://t.co/W1m1CCnN3Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 10 03:49:44 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>1127</td>\n",
       "      <td>So @aoc wants to raise our taxes and she hasn’t even paid her own! https://t.co/W1m1CCnN3Y</td>\n",
       "      <td>so aoc want to raise -PRON- tax and -PRON- hasn t even pay -PRON- own https t co w m ccnn y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465115</th>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Now @aoc says America is garbage? \\nWho elected this woman!!! https://t.co/O7hsaAK9jH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 10 22:30:39 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>1056</td>\n",
       "      <td>Now @aoc says America is garbage? \\nWho elected this woman!!! https://t.co/O7hsaAK9jH</td>\n",
       "      <td>now aoc say america be garbage who elect this woman https t co o hsaak jh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62994</th>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Former FEC Commissioner says there’s more than enough evidence to launch a criminal investigation into @aoc! https://t.co/XPL6LhrZFD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Mar 12 01:25:16 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>997</td>\n",
       "      <td>Former FEC Commissioner says there’s more than enough evidence to launch a criminal investigation into @aoc! https://t.co/XPL6LhrZFD</td>\n",
       "      <td>former fec commissioner say there s more than enough evidence to launch a criminal investigation into aoc https t co xpl lhrzfd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205837</th>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hey AOC! Guess what? Climate change and global warming IS a hoax!!!! https://t.co/OwY68EMVMm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Mar 08 05:02:07 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>838</td>\n",
       "      <td>Hey AOC! Guess what? Climate change and global warming IS a hoax!!!! https://t.co/OwY68EMVMm</td>\n",
       "      <td>hey aoc guess what climate change and global warming be a hoax https t co owy emvmm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            screen_name  followers_count  modularity_class  \\\n",
       "405812   DavidJHarrisJr            75631               0.0   \n",
       "712255   DavidJHarrisJr            75652               0.0   \n",
       "465115   DavidJHarrisJr            75638               0.0   \n",
       "62994    DavidJHarrisJr            75620               0.0   \n",
       "1205837  DavidJHarrisJr            75690               0.0   \n",
       "\n",
       "                                                                                                                                              main_text  \\\n",
       "405812   So @aoc answered a casting call... and now she’s in Congress! @RealJamesWoods and @realDonaldTrump have to see this!!! https://t.co/HBUW78XqAo   \n",
       "712255                                                       So @aoc wants to raise our taxes and she hasn’t even paid her own! https://t.co/W1m1CCnN3Y   \n",
       "465115                                                            Now @aoc says America is garbage? \\nWho elected this woman!!! https://t.co/O7hsaAK9jH   \n",
       "62994              Former FEC Commissioner says there’s more than enough evidence to launch a criminal investigation into @aoc! https://t.co/XPL6LhrZFD   \n",
       "1205837                                                    Hey AOC! Guess what? Climate change and global warming IS a hoax!!!! https://t.co/OwY68EMVMm   \n",
       "\n",
       "        retweet_text                      tweet_date  PageRank  rt_count  \\\n",
       "405812           NaN  Mon Mar 11 01:16:04 +0000 2019  0.004242      6936   \n",
       "712255           NaN  Sun Mar 10 03:49:44 +0000 2019  0.004242      1127   \n",
       "465115           NaN  Sun Mar 10 22:30:39 +0000 2019  0.004242      1056   \n",
       "62994            NaN  Tue Mar 12 01:25:16 +0000 2019  0.004242       997   \n",
       "1205837          NaN  Fri Mar 08 05:02:07 +0000 2019  0.004242       838   \n",
       "\n",
       "                                                                                                                                             final_text  \\\n",
       "405812   So @aoc answered a casting call... and now she’s in Congress! @RealJamesWoods and @realDonaldTrump have to see this!!! https://t.co/HBUW78XqAo   \n",
       "712255                                                       So @aoc wants to raise our taxes and she hasn’t even paid her own! https://t.co/W1m1CCnN3Y   \n",
       "465115                                                            Now @aoc says America is garbage? \\nWho elected this woman!!! https://t.co/O7hsaAK9jH   \n",
       "62994              Former FEC Commissioner says there’s more than enough evidence to launch a criminal investigation into @aoc! https://t.co/XPL6LhrZFD   \n",
       "1205837                                                    Hey AOC! Guess what? Climate change and global warming IS a hoax!!!! https://t.co/OwY68EMVMm   \n",
       "\n",
       "                                                                                                                         tweet_processed  \n",
       "405812    so aoc answer a cast call and now -PRON- s in congress realjameswood and realdonaldtrump have to see this https t co hbuw xqao  \n",
       "712255                                       so aoc want to raise -PRON- tax and -PRON- hasn t even pay -PRON- own https t co w m ccnn y  \n",
       "465115                                                         now aoc say america be garbage who elect this woman https t co o hsaak jh  \n",
       "62994    former fec commissioner say there s more than enough evidence to launch a criminal investigation into aoc https t co xpl lhrzfd  \n",
       "1205837                                              hey aoc guess what climate change and global warming be a hoax https t co owy emvmm  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/community_pickles_pre_nlp/preprocessed_dfs/df_all.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_all, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/community_pickles_pre_nlp/preprocessed_dfs/df_all.pickle', 'rb') as f:\n",
    "#     df_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the concat worked, the # of rows equals the number of all the rows added up (just to be safe)\n",
    "\n",
    "Now I'll run a few different LSAs and NMFs on the total data, and move on to clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text - for both LSA and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can come back to tweek this if I notice anything\n",
    "# tweet_stopwords = stopwords.words('english') + \\\n",
    "#     ['rt', 'https', 'http', 'amp', 'via', 'one', 'around', 'would', 'let', 'could', 'going', 'like', \n",
    "#      'get', 'may', 'says', 'say', 'make', 'based', 'even', 'another', 'completely', 'thanks', 'way', \n",
    "#      'find', 'used', 'thing', '2019', 'see', 'need', 'know', 'knows', 'think', 'thinks', 'take', 'new', \n",
    "#      'day', 'days', 'aoc', 'alexandria', 'ocasio', 'cortez', 'ocasio-cortez', 'pron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_all = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all = df_all['tweet_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words_all = tfidf_all.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(bag_of_words_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA on all text\n",
    "*10, 15, 20, 25 topics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284424, 0.01030946, 0.00990273, 0.00920476, 0.00827643,\n",
       "       0.00781187, 0.00793363, 0.0072784 , 0.00735234, 0.00721214])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSA on 10 topics\n",
    "# lsa_all_10 = TruncatedSVD(10)\n",
    "# lsa_all_10.fit_transform(bag_of_words_all)\n",
    "# lsa_all_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, box fraud, visit film, represent neighbor, crew postal, film news, never boyfriend, claim visit, apartment local, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, co cvfc, kjzum, cvfc, cvfc kjzum, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, scandal misinformation, complaint fox, report allege\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, climate change, inequality address, divide address, change together, together work, address income, tsbac, co tsbac, work sxsw\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, edge solve, visionary tinker, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "black, ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, co fzvfdsnue, fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n"
     ]
    }
   ],
   "source": [
    "# display_topics(lsa_all_10, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_10, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284409, 0.01030946, 0.00990181, 0.00920577, 0.00827817,\n",
       "       0.00780601, 0.00793854, 0.00728814, 0.00735668, 0.00721322,\n",
       "       0.00614608, 0.00581037, 0.00529221, 0.00522468, 0.00525943])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSA on 15 topics\n",
    "# lsa_all_15 = TruncatedSVD(15)\n",
    "# lsa_all_15.fit_transform(bag_of_words_all)\n",
    "# lsa_all_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, claim visit, visit film, crew postal, film news, represent neighbor, apartment local, never boyfriend, box fraud, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, cvfc kjzum, cvfc, co cvfc, kjzum, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, scandal misinformation, complaint fox, report allege\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, climate change, inequality address, divide address, change together, together work, address income, co tsbac, tsbac, work sxsw\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, visionary tinker, edge solve, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, co fzvfdsnue, fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n",
      "\n",
      "Topic  10\n",
      "instead, uber, subway, spend, everything, world go, go end, lyft instead, demise instead, subway fly\n",
      "\n",
      "Topic  11\n",
      "class, class american, american, reagan, working class, working, work class, screw, ronald reagan, ronald\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, msnbc, blackout cnn, ignore fec, medium blackout, msnbc ignore, kxadajq jl, co kxadajq, kxadajq, jl\n",
      "\n",
      "Topic  13\n",
      "palestinian, israel, right exist, israel right, believe israel, terrorist kill, tlaib grow, faction hama, choose palestinian, palestinian faction\n",
      "\n",
      "Topic  14\n",
      "boy, uh, necessary boy, scouts away, away boy, co xkadfguk, xkadfguk, boy co, boy scouts, necessary\n"
     ]
    }
   ],
   "source": [
    "# display_topics(lsa_all_15, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_15, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284416, 0.01031006, 0.00990201, 0.0092044 , 0.00827566,\n",
       "       0.00780765, 0.00793947, 0.00729166, 0.00735414, 0.00721279,\n",
       "       0.00615355, 0.00581353, 0.00530054, 0.0052364 , 0.0052897 ,\n",
       "       0.00525525, 0.00489424, 0.00446149, 0.00414687, 0.00423378])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSA on 20 topics\n",
    "# lsa_all_20 = TruncatedSVD(20)\n",
    "# lsa_all_20.fit_transform(bag_of_words_all)\n",
    "# lsa_all_20.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, film news, box fraud, crew postal, visit film, claim visit, apartment local, represent neighbor, never boyfriend, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, cvfc kjzum, co cvfc, cvfc, kjzum, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, scandal misinformation, complaint fox, report allege\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, climate change, inequality address, divide address, change together, together work, address income, co tsbac, tsbac, work sxsw\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, edge solve, visionary tinker, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, co fzvfdsnue, fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n",
      "\n",
      "Topic  10\n",
      "instead, uber, subway, spend, everything, world go, go end, demise instead, lyft instead, subway fly\n",
      "\n",
      "Topic  11\n",
      "class, class american, american, reagan, working class, working, work class, screw, white, ronald reagan\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, fec, msnbc, blackout cnn, ignore fec, msnbc ignore, medium blackout, fec complaint, co kxadajq, kxadajq jl\n",
      "\n",
      "Topic  13\n",
      "uh, palestinian, israel, boy, omar, right exist, israel right, believe israel, terrorist kill, tlaib grow\n",
      "\n",
      "Topic  14\n",
      "palestinian, israel, believe israel, right exist, israel right, tlaib grow, palestinian faction, choose palestinian, faction hama, pay terrorist\n",
      "\n",
      "Topic  15\n",
      "boy, necessary boy, scouts away, away boy, co xkadfguk, xkadfguk, boy co, boy scouts, necessary, scouts\n",
      "\n",
      "Topic  16\n",
      "answer cast, cast call, congress realjameswood, hbuw, hbuw xqao, xqao, co hbuw, call congress, realjameswood realdonaldtrump, cast\n",
      "\n",
      "Topic  17\n",
      "grip slip, friend throw, grandparent home, leave grandparent, slip friend, pelosi grip, throw party, party leave, grandparent, newt\n",
      "\n",
      "Topic  18\n",
      "anti, anti semitism, semitism, condemn, resolution, sham, sham resolution, design, vote, rep\n",
      "\n",
      "Topic  19\n",
      "public transportation, transportation, public, car, subway ft, ft campaign, emission spend, transportation association, transportation instead, association single\n"
     ]
    }
   ],
   "source": [
    "# display_topics(lsa_all_20, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_20, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284428, 0.01031   , 0.00990254, 0.0092052 , 0.0082777 ,\n",
       "       0.00780795, 0.00793927, 0.00729185, 0.00735446, 0.00721273,\n",
       "       0.00615261, 0.00580615, 0.00530464, 0.00523566, 0.00529585,\n",
       "       0.00525649, 0.00492341, 0.0044789 , 0.00419832, 0.00428453,\n",
       "       0.00406109, 0.00396976, 0.00393049, 0.00365576, 0.00348358])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # LSA on 25 topics\n",
    "# lsa_all_25 = TruncatedSVD(25)\n",
    "# lsa_all_25.fit_transform(bag_of_words_all)\n",
    "# lsa_all_25.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, box fraud, apartment local, represent neighbor, crew postal, never boyfriend, claim visit, film news, visit film, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, co cvfc, kjzum, cvfc, cvfc kjzum, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, scandal misinformation, complaint fox, report allege\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, climate change, inequality address, divide address, change together, together work, address income, co tsbac, tsbac, work sxsw\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, visionary tinker, edge solve, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, co fzvfdsnue, fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n",
      "\n",
      "Topic  10\n",
      "instead, uber, subway, spend, everything, world go, go end, lyft instead, demise instead, subway fly\n",
      "\n",
      "Topic  11\n",
      "class, class american, american, reagan, working class, working, work class, screw, white, ronald reagan\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, msnbc, fec, blackout cnn, ignore fec, msnbc ignore, medium blackout, co kxadajq, kxadajq, kxadajq jl\n",
      "\n",
      "Topic  13\n",
      "uh, palestinian, israel, uh uh, clapback game, perfect clapback, xatvtcm, without roasted, jh xatvtcm, roasted act\n",
      "\n",
      "Topic  14\n",
      "palestinian, israel, believe israel, right exist, israel right, tlaib grow, palestinian faction, faction hama, choose palestinian, pay terrorist\n",
      "\n",
      "Topic  15\n",
      "boy, necessary boy, scouts away, away boy, co xkadfguk, xkadfguk, boy co, boy scouts, necessary, scouts\n",
      "\n",
      "Topic  16\n",
      "answer cast, cast call, congress realjameswood, hbuw xqao, xqao, hbuw, co hbuw, call congress, realjameswood realdonaldtrump, cast\n",
      "\n",
      "Topic  17\n",
      "grip slip, friend throw, leave grandparent, grandparent home, slip friend, pelosi grip, throw party, party leave, grandparent, newt\n",
      "\n",
      "Topic  18\n",
      "anti, anti semitism, semitism, resolution, condemn, sham resolution, sham, design, anti semitic, semitic\n",
      "\n",
      "Topic  19\n",
      "public transportation, transportation, public, car, ft campaign, emission spend, association single, subway ft, instead car, car realize\n",
      "\n",
      "Topic  20\n",
      "harris, rate, multiple job, low people, work multiple, rate low, unemployment rate, kamala harris, people work, unemployment\n",
      "\n",
      "Topic  21\n",
      "state america, united state, united, america, state, harris, low, rate, multiple job, unemployment\n",
      "\n",
      "Topic  22\n",
      "tax, high tax, week, high, possible coordination, travel mom, coordination want, uber air, ton uber, list fec\n",
      "\n",
      "Topic  23\n",
      "landslide, elect, reagan, landslide elect, big landslide, elect landslide, elect big, lord brilliant, reagan elect, co aywff\n",
      "\n",
      "Topic  24\n",
      "learn, everything, economic, everything judaism, professor everything, judaism learn, learn louis, leftist college, farrakhan everything, learn leftist\n"
     ]
    }
   ],
   "source": [
    "# display_topics(lsa_all_25, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_25, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF on all text\n",
    "*10, 15, 20, 25 topics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085.5968911147454\n"
     ]
    }
   ],
   "source": [
    "# # NMF on 10 topics\n",
    "# nmf_all_10 = NMF(10)\n",
    "# nmf_all_10.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_10.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, crew postal, film news, visit film, represent neighbor, claim visit, apartment local, box fraud, never boyfriend, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, cvfc, cvfc kjzum, kjzum, co cvfc, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, news report, complaint fox, scandal misinformation\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, inequality address, divide address, change together, together work, address income, co tsbac, tsbac, work sxsw, fear divide\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, visionary tinker, edge solve, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, fzvfdsnue, co fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n"
     ]
    }
   ],
   "source": [
    "# display_topics(nmf_all_10, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_10, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068.5427287389832\n"
     ]
    }
   ],
   "source": [
    "# # NMF on 15 topics\n",
    "# nmf_all_15 = NMF(15)\n",
    "# nmf_all_15.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_15.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, claim visit, crew postal, apartment local, box fraud, visit film, represent neighbor, film news, never boyfriend, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, kjzum, co cvfc, cvfc kjzum, cvfc, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, news report, scandal misinformation, complaint fox\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, inequality address, divide address, change together, together work, address income, co tsbac, tsbac, work sxsw, fear divide\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, visionary tinker, edge solve, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, fzvfdsnue, co fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n",
      "\n",
      "Topic  10\n",
      "instead, subway, uber, spend, everything, world go, go end, demise instead, lyft instead, subway fly\n",
      "\n",
      "Topic  11\n",
      "class, class american, american, reagan, working class, working, work class, screw, ronald reagan, ronald\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, msnbc, blackout cnn, ignore fec, medium blackout, msnbc ignore, kxadajq, co kxadajq, kxadajq jl, jl\n",
      "\n",
      "Topic  13\n",
      "uh, boy, uh uh, clapback game, mess bronx, xatvtcm, though borough, without roasted, roasted act, perfect clapback\n",
      "\n",
      "Topic  14\n",
      "palestinian, israel, right exist, israel right, believe israel, terrorist kill, faction hama, palestinian faction, tlaib grow, choose palestinian\n"
     ]
    }
   ],
   "source": [
    "# display_topics(nmf_all_15, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_15, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054.2875702490992\n"
     ]
    }
   ],
   "source": [
    "# # NMF on 20 topics\n",
    "# nmf_all_20 = NMF(20)\n",
    "# nmf_all_20.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_20.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, film news, apartment local, crew postal, never boyfriend, box fraud, represent neighbor, visit film, claim visit, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, co cvfc, kjzum, cvfc kjzum, cvfc, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, news report, scandal misinformation, complaint fox\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, crazy green, oppose green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, inequality address, divide address, change together, together work, address income, tsbac, co tsbac, work sxsw, fear divide\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, visionary tinker, edge solve, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, learn, service customer, bartender long, co fzvfdsnue, fzvfdsnue, lot life, customer service, job lot\n",
      "\n",
      "Topic  9\n",
      "garbage, america, america garbage, call america, garbage congresswoman, congresswoman garbage, hellhole praise, praise venezuela, garbage citizen, force eat\n",
      "\n",
      "Topic  10\n",
      "instead, everything, world go, go end, demise instead, lyft instead, subway fly, use amtrak, slow demise, everything slow\n",
      "\n",
      "Topic  11\n",
      "class, class american, american, reagan, working class, working, work class, screw, ronald reagan, ronald\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, msnbc, blackout cnn, ignore fec, medium blackout, msnbc ignore, co kxadajq, kxadajq, kxadajq jl, jl\n",
      "\n",
      "Topic  13\n",
      "palestinian, israel, right exist, believe israel, israel right, terrorist kill, tlaib grow, faction hama, choose palestinian, palestinian faction\n",
      "\n",
      "Topic  14\n",
      "uh, uh uh, clapback game, mess bronx, though borough, perfect clapback, roasted act, jh xatvtcm, xatvtcm, without roasted\n",
      "\n",
      "Topic  15\n",
      "boy, necessary boy, scouts away, away boy, xkadfguk, co xkadfguk, boy co, boy scouts, necessary, scouts\n",
      "\n",
      "Topic  16\n",
      "answer cast, cast call, congress realjameswood, co hbuw, xqao, hbuw xqao, hbuw, call congress, realjameswood realdonaldtrump, cast\n",
      "\n",
      "Topic  17\n",
      "grip slip, friend throw, leave grandparent, grandparent home, slip friend, pelosi grip, throw party, party leave, grandparent, newt\n",
      "\n",
      "Topic  18\n",
      "anti, anti semitism, semitism, resolution, condemn, sham resolution, sham, design, omar, rep\n",
      "\n",
      "Topic  19\n",
      "public transportation, transportation, public, car, uber, spend, emission spend, transportation association, car realize, transportation instead\n"
     ]
    }
   ],
   "source": [
    "# display_topics(nmf_all_20, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_20, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042.4274847932734\n"
     ]
    }
   ],
   "source": [
    "# # NMF on 25 topics\n",
    "# nmf_all_25 = NMF(25)\n",
    "# nmf_all_25.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_25.reconstruction_err_)\n",
    "\n",
    "# ## setting a variable to the fit transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmf_model_25_df = pd.DataFrame(data=nmf_all_25.fit_transform(bag_of_words_all))\n",
    "# nmf_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042.4277109200966\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3    4         5         6         7   \\\n",
       "0  0.0  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "1  0.0  0.000341  0.000030  0.000488  0.0  0.000410  0.000302  0.000239   \n",
       "2  0.0  0.000201  0.000123  0.000141  0.0  0.000175  0.000109  0.000000   \n",
       "3  0.0  0.000227  0.000000  0.000168  0.0  0.000199  0.000243  0.002161   \n",
       "4  0.0  0.000223  0.000073  0.000072  0.0  0.007420  0.000188  0.000149   \n",
       "\n",
       "         8         9  ...         15        16        17        18        19  \\\n",
       "0  0.000000  0.000000 ...   0.000000  0.112873  0.000000  0.000000  0.000000   \n",
       "1  0.000329  0.000000 ...   0.000617  0.000773  0.000503  0.000775  0.000000   \n",
       "2  0.000000  0.010703 ...   0.000017  0.000000  0.000183  0.000109  0.000043   \n",
       "3  0.000307  0.000000 ...   0.000441  0.000516  0.000245  0.000574  0.000057   \n",
       "4  0.000092  0.000000 ...   0.000427  0.000664  0.000307  0.000473  0.000000   \n",
       "\n",
       "         20        21        22        23   24  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "1  0.000674  0.003253  0.010097  0.000656  0.0  \n",
       "2  0.000000  0.015331  0.000000  0.010695  0.0  \n",
       "3  0.000031  0.000271  0.003239  0.000439  0.0  \n",
       "4  0.000000  0.000351  0.000020  0.000372  0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(nmf_all_25.reconstruction_err_)\n",
    "# nmf_model_25_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF - if the above works, I'll need to make DFs of all versions again (which will take around another hour) :(\n",
    "\n",
    "Then I'll concat them with index and screen name, so I can assign values back to the tweeters otherwise. One annoying thing is I didn't pass in tweet ID so I'll have to think about that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "claim, apartment local, crew postal, never boyfriend, claim visit, film news, visit film, box fraud, represent neighbor, claim mail\n",
      "\n",
      "Topic  1\n",
      "skill libs, libs nice, cookie teach, cvfc, cvfc kjzum, co cvfc, kjzum, boycott cookie, nice job, teach little\n",
      "\n",
      "Topic  2\n",
      "file bogus, case conspiracy, allege untrue, spamm file, group spamm, untrue scandal, theory run, news report, scandal misinformation, complaint fox\n",
      "\n",
      "Topic  3\n",
      "society job, live society, job leave, leave die, automate work, automate, die core, core problem, excited live, reason excited\n",
      "\n",
      "Topic  4\n",
      "hawaii, crazy, green deal, green, deal, hawaii anyone, oppose green, crazy green, senator mazie, hirono hawaii\n",
      "\n",
      "Topic  5\n",
      "address, inequality address, divide address, change together, together work, address income, tsbac, co tsbac, work sxsw, fear divide\n",
      "\n",
      "Topic  6\n",
      "moderate, time rewrite, edge solve, visionary tinker, naive visionary, moderate naive, moderate spot, problem democracy, democracy economy, critique moderate\n",
      "\n",
      "Topic  7\n",
      "ilhanmn finally, folk former, hat respect, klan celebrate, wizard ku, left exactly, celebrate party, stupid jews, respect ilhanmn, exactly hateful\n",
      "\n",
      "Topic  8\n",
      "service, long, service customer, bartender long, fzvfdsnue, co fzvfdsnue, lot life, customer service, job lot, work service\n",
      "\n",
      "Topic  9\n",
      "garbage, congresswoman garbage, hellhole praise, praise venezuela, garbage congresswoman, garbage citizen, force eat, socialist hellhole, eat socialist, seriously call\n",
      "\n",
      "Topic  10\n",
      "instead, world go, go end, demise instead, lyft instead, subway fly, use amtrak, slow demise, everything slow, thousand uber\n",
      "\n",
      "Topic  11\n",
      "class american, class, american, working class, working, reagan, work class, screw, white, ronald reagan\n",
      "\n",
      "Topic  12\n",
      "cnn msnbc, msnbc, blackout cnn, ignore fec, medium blackout, msnbc ignore, co kxadajq, kxadajq jl, kxadajq, jl\n",
      "\n",
      "Topic  13\n",
      "palestinian, israel, right exist, believe israel, israel right, terrorist kill, tlaib grow, faction hama, choose palestinian, palestinian faction\n",
      "\n",
      "Topic  14\n",
      "uh, uh uh, clapback game, mess bronx, perfect clapback, xatvtcm, roasted act, jh xatvtcm, without roasted, though borough\n",
      "\n",
      "Topic  15\n",
      "boy, necessary boy, scouts away, away boy, xkadfguk, co xkadfguk, boy co, boy scouts, necessary, scouts\n",
      "\n",
      "Topic  16\n",
      "answer cast, cast call, congress realjameswood, hbuw, hbuw xqao, xqao, co hbuw, call congress, realjameswood realdonaldtrump, cast\n",
      "\n",
      "Topic  17\n",
      "grip slip, friend throw, grandparent home, leave grandparent, slip friend, pelosi grip, throw party, party leave, grandparent, newt\n",
      "\n",
      "Topic  18\n",
      "anti, anti semitism, semitism, resolution, condemn, sham resolution, sham, design, omar, anti semitic\n",
      "\n",
      "Topic  19\n",
      "public transportation, transportation, public, transportation instead, car realize, transportation association, instead car, subway ft, ft campaign, association single\n",
      "\n",
      "Topic  20\n",
      "harris, claim, multiple job, work multiple, low people, rate low, unemployment rate, kamala harris, people work, rate\n",
      "\n",
      "Topic  21\n",
      "america, state america, great, garbage, united state, united, state, america garbage, firsthand united, immigrate united\n",
      "\n",
      "Topic  22\n",
      "high tax, tax, week, high, want, uber air, travel mom, coordination want, ton uber, possible coordination\n",
      "\n",
      "Topic  23\n",
      "landslide, elect, landslide elect, elect landslide, big landslide, elect big, lord brilliant, co aywff, landslide bu, aywff tpdy\n",
      "\n",
      "Topic  24\n",
      "learn, everything, economic, everything judaism, judaism learn, professor everything, learn louis, leftist college, farrakhan everything, learn leftist\n"
     ]
    }
   ],
   "source": [
    "# display_topics(nmf_all_25, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_25, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create DF's where I append this info to a dataframe\n",
    "\n",
    "then try dbscan with these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>modularity_class</th>\n",
       "      <th>main_text</th>\n",
       "      <th>retweet_text</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>PageRank</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>final_text</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>405812</td>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So @aoc answered a casting call... and now she...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Mar 11 01:16:04 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>6936</td>\n",
       "      <td>So @aoc answered a casting call... and now she...</td>\n",
       "      <td>so aoc answer a cast call and now -PRON- s in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>712255</td>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So @aoc wants to raise our taxes and she hasn’...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 10 03:49:44 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>1127</td>\n",
       "      <td>So @aoc wants to raise our taxes and she hasn’...</td>\n",
       "      <td>so aoc want to raise -PRON- tax and -PRON- has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>465115</td>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Now @aoc says America is garbage? \\nWho electe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Mar 10 22:30:39 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>1056</td>\n",
       "      <td>Now @aoc says America is garbage? \\nWho electe...</td>\n",
       "      <td>now aoc say america be garbage who elect this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62994</td>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Former FEC Commissioner says there’s more than...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Mar 12 01:25:16 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>997</td>\n",
       "      <td>Former FEC Commissioner says there’s more than...</td>\n",
       "      <td>former fec commissioner say there s more than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1205837</td>\n",
       "      <td>DavidJHarrisJr</td>\n",
       "      <td>75690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hey AOC! Guess what? Climate change and global...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Mar 08 05:02:07 +0000 2019</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>838</td>\n",
       "      <td>Hey AOC! Guess what? Climate change and global...</td>\n",
       "      <td>hey aoc guess what climate change and global w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     screen_name  followers_count  modularity_class  \\\n",
       "0   405812  DavidJHarrisJr            75631               0.0   \n",
       "1   712255  DavidJHarrisJr            75652               0.0   \n",
       "2   465115  DavidJHarrisJr            75638               0.0   \n",
       "3    62994  DavidJHarrisJr            75620               0.0   \n",
       "4  1205837  DavidJHarrisJr            75690               0.0   \n",
       "\n",
       "                                           main_text retweet_text  \\\n",
       "0  So @aoc answered a casting call... and now she...          NaN   \n",
       "1  So @aoc wants to raise our taxes and she hasn’...          NaN   \n",
       "2  Now @aoc says America is garbage? \\nWho electe...          NaN   \n",
       "3  Former FEC Commissioner says there’s more than...          NaN   \n",
       "4  Hey AOC! Guess what? Climate change and global...          NaN   \n",
       "\n",
       "                       tweet_date  PageRank  rt_count  \\\n",
       "0  Mon Mar 11 01:16:04 +0000 2019  0.004242      6936   \n",
       "1  Sun Mar 10 03:49:44 +0000 2019  0.004242      1127   \n",
       "2  Sun Mar 10 22:30:39 +0000 2019  0.004242      1056   \n",
       "3  Tue Mar 12 01:25:16 +0000 2019  0.004242       997   \n",
       "4  Fri Mar 08 05:02:07 +0000 2019  0.004242       838   \n",
       "\n",
       "                                          final_text  \\\n",
       "0  So @aoc answered a casting call... and now she...   \n",
       "1  So @aoc wants to raise our taxes and she hasn’...   \n",
       "2  Now @aoc says America is garbage? \\nWho electe...   \n",
       "3  Former FEC Commissioner says there’s more than...   \n",
       "4  Hey AOC! Guess what? Climate change and global...   \n",
       "\n",
       "                                     tweet_processed  \n",
       "0  so aoc answer a cast call and now -PRON- s in ...  \n",
       "1  so aoc want to raise -PRON- tax and -PRON- has...  \n",
       "2  now aoc say america be garbage who elect this ...  \n",
       "3  former fec commissioner say there s more than ...  \n",
       "4  hey aoc guess what climate change and global w...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_topic_base = df_all.reset_index()\n",
    "# df_topic_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crap - now I know why I saved the fit_transform(bag_of_words_ objects previously, that's what I need to use to create the df. For now I'm seeing if I can just call that as the \"data\" and see if that works. Otherwise I will need to set them all equal to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 11)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_topic_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsa_all_10 = TruncatedSVD(10)\n",
    "\n",
    "# lsa_model_10_df = pd.DataFrame(data=lsa_all_10.fit_transform(bag_of_words_all))\n",
    "# lsa_model_10_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 15)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsa_all_15 = TruncatedSVD(15)\n",
    "\n",
    "# lsa_model_15_df = pd.DataFrame(data=lsa_all_15.fit_transform(bag_of_words_all))\n",
    "# lsa_model_15_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsa_all_20 = TruncatedSVD(20)\n",
    "\n",
    "# lsa_model_20_df = pd.DataFrame(data=lsa_all_20.fit_transform(bag_of_words_all))\n",
    "# lsa_model_20_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsa_all_25 = TruncatedSVD(25)\n",
    "\n",
    "# lsa_model_25_df = pd.DataFrame(data=lsa_all_25.fit_transform(bag_of_words_all))\n",
    "# lsa_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lsa_model_10 = pd.merge(df_topic_base, lsa_model_10_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_15 = pd.merge(df_topic_base, lsa_model_15_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_20 = pd.merge(df_topic_base, lsa_model_20_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_25 = pd.merge(df_topic_base, lsa_model_25_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmf_all_10 = NMF(10)\n",
    "\n",
    "# nmf_model_10_df = pd.DataFrame(data=nmf_all_10.fit_transform(bag_of_words_all))\n",
    "# nmf_model_10_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmf_all_15 = NMF(15)\n",
    "\n",
    "# nmf_model_15_df = pd.DataFrame(data=nmf_all_15.fit_transform(bag_of_words_all))\n",
    "# nmf_model_15_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296014, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nmf_all_20 = NMF(20)\n",
    "\n",
    "# nmf_model_20_df = pd.DataFrame(data=nmf_all_20.fit_transform(bag_of_words_all))\n",
    "# nmf_model_20_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nmf_model_25_df = pd.DataFrame(data=nmf_all_25.fit_transform(bag_of_words_all)) ALREADY DONE!\n",
    "# nmf_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nmf_model_10 = pd.merge(df_topic_base, nmf_model_10_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_15 = pd.merge(df_topic_base, nmf_model_15_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_20 = pd.merge(df_topic_base, nmf_model_20_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_25 = pd.merge(df_topic_base, nmf_model_25_df, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_10, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_15, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_20, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_25, to_write)\n",
    "    \n",
    "\n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_10, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_15, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_20, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_25, to_write)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
