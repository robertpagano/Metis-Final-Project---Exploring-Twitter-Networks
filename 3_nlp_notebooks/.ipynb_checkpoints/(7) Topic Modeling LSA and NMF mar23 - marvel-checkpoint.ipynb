{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited version of AOC to make it more organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)  # or 199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, preprocessing (using same code as for kickstarter as baseline, come back here to tweak later)\n",
    "\n",
    "Look into https://pypi.org/project/tweet-preprocessor/ for tweet processing later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #lemmatize with Spacy\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_stopwords = stopwords.words('english') + \\\n",
    "    ['rt', 'https', 'http', 'amp', 'via', 'one', 'around', 'would', 'let', 'could', 'going', 'like', \n",
    "     'get', 'may', 'says', 'say', 'make', 'based', 'even', 'another', 'completely', 'thanks', 'way', \n",
    "     'find', 'used', 'thing', '2019', 'see', 'need', 'know', 'knows', 'think', 'thinks', 'take', 'new', \n",
    "     'day', 'days', 'captain', 'marvel', 'mcu', 'pron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_0.pickle', 'rb') as f:\n",
    "    df_0 = pickle.load(f)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_1.pickle', 'rb') as f:\n",
    "    df_1 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_2.pickle', 'rb') as f:\n",
    "    df_2 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_3.pickle', 'rb') as f:\n",
    "    df_3 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_4.pickle', 'rb') as f:\n",
    "    df_4 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_5.pickle', 'rb') as f:\n",
    "    df_5 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_6.pickle', 'rb') as f:\n",
    "    df_6 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_7.pickle', 'rb') as f:\n",
    "    df_7 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_11.pickle', 'rb') as f:\n",
    "    df_11 = pickle.load(f)\n",
    "    \n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/df_15.pickle', 'rb') as f:\n",
    "    df_15 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['final_text'] = np.where((df_0['retweet_text'].isnull() == False), df_0['retweet_text'], df_0['main_text'])\n",
    "df_1['final_text'] = np.where((df_1['retweet_text'].isnull() == False), df_1['retweet_text'], df_1['main_text'])\n",
    "df_2['final_text'] = np.where((df_2['retweet_text'].isnull() == False), df_2['retweet_text'], df_2['main_text'])\n",
    "df_3['final_text'] = np.where((df_3['retweet_text'].isnull() == False), df_3['retweet_text'], df_3['main_text'])\n",
    "df_4['final_text'] = np.where((df_4['retweet_text'].isnull() == False), df_4['retweet_text'], df_4['main_text'])\n",
    "df_5['final_text'] = np.where((df_5['retweet_text'].isnull() == False), df_5['retweet_text'], df_5['main_text'])\n",
    "df_6['final_text'] = np.where((df_6['retweet_text'].isnull() == False), df_6['retweet_text'], df_6['main_text'])\n",
    "df_7['final_text'] = np.where((df_7['retweet_text'].isnull() == False), df_7['retweet_text'], df_7['main_text'])\n",
    "df_11['final_text'] = np.where((df_11['retweet_text'].isnull() == False), df_11['retweet_text'], df_11['main_text'])\n",
    "df_15['final_text'] = np.where((df_15['retweet_text'].isnull() == False), df_15['retweet_text'], df_15['main_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN ALL BELOW ONCE PREV IS FINISHED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['tweet_processed'] = df_0['final_text'].apply(lambda x:pre_process(x))\n",
    "df_1['tweet_processed'] = df_1['final_text'].apply(lambda x:pre_process(x))\n",
    "df_2['tweet_processed'] = df_2['final_text'].apply(lambda x:pre_process(x))\n",
    "df_3['tweet_processed'] = df_3['final_text'].apply(lambda x:pre_process(x))\n",
    "df_4['tweet_processed'] = df_4['final_text'].apply(lambda x:pre_process(x))\n",
    "df_5['tweet_processed'] = df_5['final_text'].apply(lambda x:pre_process(x))\n",
    "df_6['tweet_processed'] = df_6['final_text'].apply(lambda x:pre_process(x))\n",
    "df_7['tweet_processed'] = df_7['final_text'].apply(lambda x:pre_process(x))\n",
    "df_11['tweet_processed'] = df_11['final_text'].apply(lambda x:pre_process(x))\n",
    "df_15['tweet_processed'] = df_15['final_text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_0_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_0, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_1_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_1, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_2_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_2, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_3_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_3, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_4_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_4, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_5_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_5, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_6_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_6, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_7_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_7, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_11_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_11, to_write)\n",
    "\n",
    "with open('/Users/robertpagano/src/metis_project_kojak/network_files/marvel/community_pickles_pre_nlp/preproccessed_text/df_15_preprocess.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_15, to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = df_1['tweet_processed']\n",
    "X_1 = df_1['tweet_processed']\n",
    "X_2 = df_2['tweet_processed']\n",
    "X_3 = df_3['tweet_processed']\n",
    "X_4 = df_4['tweet_processed']\n",
    "X_5 = df_5['tweet_processed']\n",
    "X_6 = df_6['tweet_processed']\n",
    "X_7 = df_7['tweet_processed']\n",
    "X_11 = df_11['tweet_processed']\n",
    "X_15 = df_15['tweet_processed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_0 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_1 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_2 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_3 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_4 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_5 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_6 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_7 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_11 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n",
    "tfidf_15 = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_0 = tfidf_0.fit_transform(X_0)\n",
    "lsa_0 = TruncatedSVD(20)\n",
    "doc_topic_0 = lsa_0.fit_transform(bag_of_words_0)\n",
    "lsa_0.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_0, tfidf_0.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_1 = tfidf_1.fit_transform(X_1)\n",
    "lsa_1 = TruncatedSVD(20)\n",
    "doc_topic_1 = lsa_1.fit_transform(bag_of_words_1)\n",
    "lsa_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_1, tfidf_1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_2 = tfidf_2.fit_transform(X_2)\n",
    "lsa_2 = TruncatedSVD(20)\n",
    "doc_topic_2 = lsa_2.fit_transform(bag_of_words_2)\n",
    "lsa_2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_2, tfidf_2.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_3 = tfidf_3.fit_transform(X_3)\n",
    "lsa_3 = TruncatedSVD(20)\n",
    "doc_topic_3 = lsa_3.fit_transform(bag_of_words_3)\n",
    "lsa_3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_3, tfidf_3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_4 = tfidf_4.fit_transform(X_4)\n",
    "lsa_4 = TruncatedSVD(20)\n",
    "doc_topic_4 = lsa_4.fit_transform(bag_of_words_4)\n",
    "lsa_4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_4, tfidf_4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_5 = tfidf_5.fit_transform(X_5)\n",
    "lsa_5 = TruncatedSVD(20)\n",
    "doc_topic_5 = lsa_5.fit_transform(bag_of_words_5)\n",
    "lsa_5.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_5, tfidf_5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_6 = tfidf_6.fit_transform(X_6)\n",
    "lsa_6 = TruncatedSVD(20)\n",
    "doc_topic_6 = lsa_6.fit_transform(bag_of_words_6)\n",
    "lsa_6.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_6, tfidf_6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_7 = tfidf_7.fit_transform(X_7)\n",
    "lsa_7 = TruncatedSVD(20)\n",
    "doc_topic_7 = lsa_7.fit_transform(bag_of_words_7)\n",
    "lsa_7.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_7, tfidf_7.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_11 = tfidf_11.fit_transform(X_11)\n",
    "lsa_11 = TruncatedSVD(20)\n",
    "doc_topic_11 = lsa_11.fit_transform(bag_of_words_11)\n",
    "lsa_11.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_11, tfidf_11.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_15 = tfidf_15.fit_transform(X_15)\n",
    "lsa_15 = TruncatedSVD(20)\n",
    "doc_topic_15 = lsa_15.fit_transform(bag_of_words_15)\n",
    "lsa_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa_15, tfidf_15.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have my top words in each LSA object w/ 20 topics each. I'm going to pickle each model and then use a new file to:\n",
    " - Find which tweets score the highest in each topic to get examples\n",
    " - Name communities\n",
    " - Sentiment analysis (maybe weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_0.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_0, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_1.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_1, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_2.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_2, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_3.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_3, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_4.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_4, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_5.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_5, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_6.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_6, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_7.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_7, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_11.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_11, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_marvel/lsa_15.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lsa_15, to_write)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF Models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nmf_0 = NMF(20)\n",
    "doc_topic_NMF_0 = nmf_0.fit_transform(bag_of_words_0)\n",
    "print(nmf_0.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_0, tfidf_0.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_1 = NMF(20)\n",
    "doc_topic_NMF_1 = nmf_1.fit_transform(bag_of_words_1)\n",
    "print(nmf_1.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_1, tfidf_1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_2 = NMF(20)\n",
    "doc_topic_NMF_2 = nmf_2.fit_transform(bag_of_words_2)\n",
    "print(nmf_2.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_2, tfidf_2.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_3 = NMF(20)\n",
    "doc_topic_NMF_3 = nmf_3.fit_transform(bag_of_words_3)\n",
    "print(nmf_3.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_3, tfidf_3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_4 = NMF(20)\n",
    "doc_topic_NMF_4 = nmf_4.fit_transform(bag_of_words_4)\n",
    "print(nmf_4.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_4, tfidf_4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_5 = NMF(20)\n",
    "doc_topic_NMF_5 = nmf_5.fit_transform(bag_of_words_5)\n",
    "print(nmf_5.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_5, tfidf_5.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_6 = NMF(20)\n",
    "doc_topic_NMF_6 = nmf_6.fit_transform(bag_of_words_6)\n",
    "print(nmf_6.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_6, tfidf_6.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_7 = NMF(20)\n",
    "doc_topic_NMF_7 = nmf_7.fit_transform(bag_of_words_7)\n",
    "print(nmf_7.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_7, tfidf_7.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_11 = NMF(20)\n",
    "doc_topic_NMF_11 = nmf_11.fit_transform(bag_of_words_11)\n",
    "print(nmf_11.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_11, tfidf_11.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_15 = NMF(20)\n",
    "doc_topic_NMF_15 = nmf_15.fit_transform(bag_of_words_15)\n",
    "print(nmf_15.reconstruction_err_)\n",
    "\n",
    "display_topics(nmf_15, tfidf_15.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_0.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_0, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_1.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_1, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_2.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_2, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_3.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_3, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_4.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_4, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_5.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_5, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_6.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_6, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_7.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_7, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_11.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_11, to_write)\n",
    "    \n",
    "with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_marvel/nmf_12.pickle', 'wb') as to_write:\n",
    "    pickle.dump(nmf_12, to_write)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Topic Modeling on ALL text data, both NMF and LSA, on various topic counts, to feed into DBScan / KMeans for cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Creating list of the processed text files to merge, so it's all in the same format and can re-use code\n",
    "# dfs_to_merge = [df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_26, df_40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "\n",
    "# for df in dfs_to_merge:\n",
    "#     print(df.shape)\n",
    "#     total += df.shape[0]\n",
    "    \n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_preprocessed = pd.concat(dfs_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_all_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = df_all_preprocessed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/community_pickles_pre_nlp/preprocessed_dfs/df_all.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_all, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/community_pickles_pre_nlp/preprocessed_dfs/df_all.pickle', 'rb') as f:\n",
    "#     df_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the concat worked, the # of rows equals the number of all the rows added up (just to be safe)\n",
    "\n",
    "Now I'll run a few different LSAs and NMFs on the total data, and move on to clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text - for both LSA and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can come back to tweek this if I notice anything\n",
    "# tweet_stopwords = stopwords.words('english') + \\\n",
    "#     ['rt', 'https', 'http', 'amp', 'via', 'one', 'around', 'would', 'let', 'could', 'going', 'like', \n",
    "#      'get', 'may', 'says', 'say', 'make', 'based', 'even', 'another', 'completely', 'thanks', 'way', \n",
    "#      'find', 'used', 'thing', '2019', 'see', 'need', 'know', 'knows', 'think', 'thinks', 'take', 'new', \n",
    "#      'day', 'days', 'aoc', 'alexandria', 'ocasio', 'cortez', 'ocasio-cortez', 'pron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_all = TfidfVectorizer(stop_words=tweet_stopwords, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all = df_all['tweet_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words_all = tfidf_all.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(bag_of_words_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA on all text\n",
    "*10, 15, 20, 25 topics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSA on 10 topics\n",
    "# lsa_all_10 = TruncatedSVD(10)\n",
    "# lsa_all_10.fit_transform(bag_of_words_all)\n",
    "# lsa_all_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display_topics(lsa_all_10, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_10, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSA on 15 topics\n",
    "# lsa_all_15 = TruncatedSVD(15)\n",
    "# lsa_all_15.fit_transform(bag_of_words_all)\n",
    "# lsa_all_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(lsa_all_15, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_15, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSA on 20 topics\n",
    "# lsa_all_20 = TruncatedSVD(20)\n",
    "# lsa_all_20.fit_transform(bag_of_words_all)\n",
    "# lsa_all_20.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(lsa_all_20, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_20, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSA on 25 topics\n",
    "# lsa_all_25 = TruncatedSVD(25)\n",
    "# lsa_all_25.fit_transform(bag_of_words_all)\n",
    "# lsa_all_25.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(lsa_all_25, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/lsa_aoc/lsa_all_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(lsa_all_25, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF on all text\n",
    "*10, 15, 20, 25 topics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF on 10 topics\n",
    "# nmf_all_10 = NMF(10)\n",
    "# nmf_all_10.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_10.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(nmf_all_10, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_10, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF on 15 topics\n",
    "# nmf_all_15 = NMF(15)\n",
    "# nmf_all_15.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_15.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(nmf_all_15, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_15, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF on 20 topics\n",
    "# nmf_all_20 = NMF(20)\n",
    "# nmf_all_20.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_20.reconstruction_err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(nmf_all_20, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_20, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF on 25 topics\n",
    "# nmf_all_25 = NMF(25)\n",
    "# nmf_all_25.fit_transform(bag_of_words_all)\n",
    "# print(nmf_all_25.reconstruction_err_)\n",
    "\n",
    "# ## setting a variable to the fit transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_model_25_df = pd.DataFrame(data=nmf_all_25.fit_transform(bag_of_words_all))\n",
    "# nmf_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nmf_all_25.reconstruction_err_)\n",
    "# nmf_model_25_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF - if the above works, I'll need to make DFs of all versions again (which will take around another hour) :(\n",
    "\n",
    "Then I'll concat them with index and screen name, so I can assign values back to the tweeters otherwise. One annoying thing is I didn't pass in tweet ID so I'll have to think about that too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_topics(nmf_all_25, tfidf_all.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/nmf_aoc/nmf_all_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(nmf_all_25, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create DF's where I append this info to a dataframe\n",
    "\n",
    "then try dbscan with these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_base = df_all.reset_index()\n",
    "# df_topic_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crap - now I know why I saved the fit_transform(bag_of_words_ objects previously, that's what I need to use to create the df. For now I'm seeing if I can just call that as the \"data\" and see if that works. Otherwise I will need to set them all equal to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_all_10 = TruncatedSVD(10)\n",
    "\n",
    "# lsa_model_10_df = pd.DataFrame(data=lsa_all_10.fit_transform(bag_of_words_all))\n",
    "# lsa_model_10_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_all_15 = TruncatedSVD(15)\n",
    "\n",
    "# lsa_model_15_df = pd.DataFrame(data=lsa_all_15.fit_transform(bag_of_words_all))\n",
    "# lsa_model_15_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_all_20 = TruncatedSVD(20)\n",
    "\n",
    "# lsa_model_20_df = pd.DataFrame(data=lsa_all_20.fit_transform(bag_of_words_all))\n",
    "# lsa_model_20_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_all_25 = TruncatedSVD(25)\n",
    "\n",
    "# lsa_model_25_df = pd.DataFrame(data=lsa_all_25.fit_transform(bag_of_words_all))\n",
    "# lsa_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lsa_model_10 = pd.merge(df_topic_base, lsa_model_10_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_15 = pd.merge(df_topic_base, lsa_model_15_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_20 = pd.merge(df_topic_base, lsa_model_20_df, left_index=True, right_index=True)\n",
    "# df_lsa_model_25 = pd.merge(df_topic_base, lsa_model_25_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_all_10 = NMF(10)\n",
    "\n",
    "# nmf_model_10_df = pd.DataFrame(data=nmf_all_10.fit_transform(bag_of_words_all))\n",
    "# nmf_model_10_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_all_15 = NMF(15)\n",
    "\n",
    "# nmf_model_15_df = pd.DataFrame(data=nmf_all_15.fit_transform(bag_of_words_all))\n",
    "# nmf_model_15_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_all_20 = NMF(20)\n",
    "\n",
    "# nmf_model_20_df = pd.DataFrame(data=nmf_all_20.fit_transform(bag_of_words_all))\n",
    "# nmf_model_20_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nmf_model_25_df = pd.DataFrame(data=nmf_all_25.fit_transform(bag_of_words_all)) ALREADY DONE!\n",
    "# nmf_model_25_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nmf_model_10 = pd.merge(df_topic_base, nmf_model_10_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_15 = pd.merge(df_topic_base, nmf_model_15_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_20 = pd.merge(df_topic_base, nmf_model_20_df, left_index=True, right_index=True)\n",
    "# df_nmf_model_25 = pd.merge(df_topic_base, nmf_model_25_df, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_10, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_15, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_20, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_lsa_model_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_lsa_model_25, to_write)\n",
    "    \n",
    "\n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_10.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_10, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_15.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_15, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_20.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_20, to_write)\n",
    "    \n",
    "# with open('/Users/robertpagano/metis_data/project_5/topic_modeling/topic_dfs/df_nmf_model_25.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(df_nmf_model_25, to_write)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
